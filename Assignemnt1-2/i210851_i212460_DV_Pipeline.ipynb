{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Analysis & Visualization Assignment # 0\n",
        "# Muhammad Kashif\n",
        "# 21i-0851\n",
        "# -----------------------------------------\n",
        "# Muhammad Huzaifa\n",
        "# 21i-2460"
      ],
      "metadata": {
        "id": "s-VBrYMF2sro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up & Downloading All Necessary Libraries & Datasets"
      ],
      "metadata": {
        "id": "ybDFMZ8K3J_8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtBA1GcT2iqN",
        "outputId": "991bb05a-84f3-4bac-cf3e-5902e1844f52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "# Install Java and Spark\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -qO - https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz | tar xz\n",
        "!pip install findspark pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the Amazon Review Dataset\n",
        "!wget https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFiles/All_Amazon_Review.json.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDNsmFj63VVt",
        "outputId": "78cd11ec-7b8d-43a5-b0ea-6882523fa4a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-02 10:25:49--  https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFiles/All_Amazon_Review.json.gz\n",
            "Resolving datarepo.eng.ucsd.edu (datarepo.eng.ucsd.edu)... 132.239.8.30\n",
            "Connecting to datarepo.eng.ucsd.edu (datarepo.eng.ucsd.edu)|132.239.8.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35974228440 (34G) [application/x-gzip]\n",
            "Saving to: ‘All_Amazon_Review.json.gz’\n",
            "\n",
            "All_Amazon_Review.j 100%[===================>]  33.50G  13.0MB/s    in 51m 48s \n",
            "\n",
            "2025-03-02 11:17:38 (11.0 MB/s) - ‘All_Amazon_Review.json.gz’ saved [35974228440/35974228440]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All Libraries"
      ],
      "metadata": {
        "id": "HWa9nLlmedNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pyspark Function Modules\n",
        "from pyspark.sql.functions import approx_count_distinct\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import sum\n",
        "\n",
        "# Pyspark Types\n",
        "from pyspark.sql.types import StructField\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.types import BooleanType\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.types import StructType\n",
        "\n",
        "# Pyspark Modules\n",
        "from pyspark.sql import SparkSession\n",
        "import findspark\n",
        "\n",
        "# Google Colab Specific Library\n",
        "from google.colab import files\n",
        "\n",
        "# Other Necessary Libraries\n",
        "import logging\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "00apOY3iecIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All Declarations"
      ],
      "metadata": {
        "id": "2KSIlZ4TgHTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/All_Amazon_Review.json.gz\""
      ],
      "metadata": {
        "id": "v6FqpceEgIj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All Functions"
      ],
      "metadata": {
        "id": "ack5Y8s2fQc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Functions"
      ],
      "metadata": {
        "id": "lYbIE5BMfZIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Logging to a File\n",
        "def setup_logging():\n",
        "    log_filename = \"logs.txt\"\n",
        "    logging.basicConfig(filename=log_filename, level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "    return log_filename\n",
        "\n",
        "# Set up Spark Environment\n",
        "def setup_spark():\n",
        "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "    os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "    findspark.init()\n",
        "\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"AmazonReviews\") \\\n",
        "        .config(\"spark.driver.memory\", \"4g\") \\\n",
        "        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    logging.info(\"Spark Session Created\")\n",
        "    return spark"
      ],
      "metadata": {
        "id": "iKcByfdCfaim"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility Functions"
      ],
      "metadata": {
        "id": "-pdkzl-Dfazv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Schema for Faster Loading\n",
        "def get_schema():\n",
        "    return StructType([\n",
        "        StructField(\"reviewerID\", StringType(), True),\n",
        "        StructField(\"asin\", StringType(), True),\n",
        "        StructField(\"reviewText\", StringType(), True),\n",
        "        StructField(\"overall\", IntegerType(), True),\n",
        "        StructField(\"unixReviewTime\", IntegerType(), True),\n",
        "        StructField(\"summary\", StringType(), True),\n",
        "        StructField(\"verified\", BooleanType(), True),\n",
        "        StructField(\"vote\", StringType(), True),\n",
        "        StructField(\"reviewTime\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "# Load Data with Logging & Error Handling\n",
        "def load_data(spark, file_path, schema):\n",
        "    start_time = time.time()\n",
        "    logging.info(f\"Data loading started at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}\")\n",
        "\n",
        "    try:\n",
        "        df = spark.read.json(file_path, schema=schema, mode=\"PERMISSIVE\")\n",
        "        record_count = df.count()\n",
        "\n",
        "        logging.info(f\"Data successfully loaded! Total records: {record_count}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during loading: {e}\")\n",
        "        return None, 0\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    logging.info(f\"Data loading completed at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}\")\n",
        "    logging.info(f\"Total time taken: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "    return df, record_count\n",
        "\n",
        "# Partition Data for Performance\n",
        "def partition_data(df):\n",
        "    partitioned_path = \"partitioned_reviews\"\n",
        "\n",
        "    # Partitioning by 'overall' rating\n",
        "    df.write.mode(\"overwrite\").partitionBy(\"overall\").parquet(partitioned_path)\n",
        "\n",
        "    logging.info(f\"Data partitioned and saved to: {partitioned_path}\")\n",
        "    print(f\"Partitioned data saved at: {partitioned_path}\")\n",
        "\n",
        "# Save Logs to a File in Colab\n",
        "def download_logs(log_filename):\n",
        "    print(\"\\nDownloading log file...\")\n",
        "    from google.colab import files\n",
        "    files.download(log_filename)"
      ],
      "metadata": {
        "id": "tjS5F4nVfdaj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Driver Function"
      ],
      "metadata": {
        "id": "-IQa5RQWfdt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate Data + Memory Usage Stats\n",
        "def validate_data(df):\n",
        "    logging.info(\"Validating data...\")\n",
        "\n",
        "    # Display sample rows\n",
        "    df.show(5, truncate=False)\n",
        "\n",
        "    # Check for null values in key columns\n",
        "    important_columns = [\"reviewerID\", \"asin\", \"reviewText\", \"overall\"]\n",
        "    null_counts = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in important_columns])\n",
        "\n",
        "    print(\"\\nChecking for Null Values:\")\n",
        "    null_counts.show()\n",
        "\n",
        "    # Count malformed records\n",
        "    corrupt_records = df.filter(df.reviewerID.isNull() & df.asin.isNull() & df.reviewText.isNull())\n",
        "    corrupt_count = corrupt_records.count()\n",
        "\n",
        "    logging.info(f\"Number of malformed records: {corrupt_count}\")\n",
        "\n",
        "    # 🆕 Approximate memory usage\n",
        "    bytes_per_record = 100  # Rough estimation\n",
        "    estimated_size_mb = (df.count() * bytes_per_record) / (1024 * 1024)\n",
        "\n",
        "    logging.info(f\"Estimated DataFrame Size: {estimated_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "id": "zNB4-p8s06vR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "8RH_QN1jfzYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up Spark & Schema"
      ],
      "metadata": {
        "id": "s8rAjsmuf0vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_filename = setup_logging()\n",
        "spark = setup_spark()\n",
        "schema = get_schema()"
      ],
      "metadata": {
        "id": "ZxOHmXUZ06tP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Data & Displaying The First 5 Rows"
      ],
      "metadata": {
        "id": "qFwiw3Q2gAT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df, record_count = load_data(spark, file_path, schema)\n",
        "\n",
        "print(record_count, \"records are found!\")\n",
        "\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xtovq2kt220W",
        "outputId": "79e98017-e17e-43b2-b601-720e65d04833"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "233055327 records are found!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(reviewerID='A27BTSGLXK2C5K', asin='B017O9P72A', reviewText=\"Alexa is not able to control my lights. If I ask her to tell me what LIFX can do, she will give me an example with one of my group names. If I use that exact same group name in a new request, she'll await that she doesn't recognize the name. This skill is VERY buggy and has not yet worked for me. I even rest Alexa, uninstalled LIFX, and set everything up again.\", overall=None, unixReviewTime=1449792000, summary=\"VERY Buggy, doesn't work.\", verified=False, vote=None, reviewTime='12 11, 2015'),\n",
              " Row(reviewerID='A27ZJ1NCBFP1HZ', asin='B017O9P72A', reviewText='Alexa works great for me so far, but I\\'m also only controlling a single bulb at the moment. Turning on/off, changing colors and adjusting brightness are all easy and quick. That being said, I\\'m expecting complications as I add more bulbs (hope for the best prepare for the worst, right?)\\n\\nI\\'d speculate that some other users\\' frustrations might stem from Alexa not recognizing their bulb or room names. After simplifying my bulb name to \\'Lamp\\' and listing it under a Living Room group (within the LIFx app), I\\'ve been able to address it by either category pretty consistently.\\n\"Turn on/off living room lights.\"\\n\"Change lamp light to [color]\"\\n\"Dim living room lights to [X]%\"\\n\\nLike any new tech, you can expect growing pains and bugs early on. Be patient. This skill isn\\'t perfect by any means, but I\\'d say it\\'s off to a decent start', overall=None, unixReviewTime=1449532800, summary='So Far So Good', verified=False, vote='5', reviewTime='12 8, 2015'),\n",
              " Row(reviewerID='ACCQIOZMFN4UK', asin='B017O9P72A', reviewText=\"Weak!!\\n\\nAlexa doesn't even recognize the name Lifx.\\nIt's a waste of time to even ask her to turn on and off lights\", overall=None, unixReviewTime=1449446400, summary='Time waster', verified=False, vote='11', reviewTime='12 7, 2015'),\n",
              " Row(reviewerID='A3KUPJ396OQF78', asin='B017O9P72A', reviewText='Can only control one of two bulbs from one of two echos', overall=None, unixReviewTime=1449273600, summary='Buggy', verified=False, vote=None, reviewTime='12 5, 2015'),\n",
              " Row(reviewerID='A1U1RE1ZI19E1H', asin='B017O9P72A', reviewText='this worked great then randomly stopped. please update.', overall=None, unixReviewTime=1517529600, summary='stopped working', verified=False, vote='2', reviewTime='02 2, 2018')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validating The Data & Applying Partitioning"
      ],
      "metadata": {
        "id": "ABv3-Rp4gLPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if df is not None and record_count > 0:\n",
        "    validate_data(df)\n",
        "    partition_data(df)  # Partitioning for performance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ngSnZBm-22yJ",
        "outputId": "8fd1296e-c73b-4f1a-9b38-849027c51329"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+--------------+-------------------------+--------+----+-----------+\n",
            "|reviewerID    |asin      |reviewText                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |overall|unixReviewTime|summary                  |verified|vote|reviewTime |\n",
            "+--------------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+--------------+-------------------------+--------+----+-----------+\n",
            "|A27BTSGLXK2C5K|B017O9P72A|Alexa is not able to control my lights. If I ask her to tell me what LIFX can do, she will give me an example with one of my group names. If I use that exact same group name in a new request, she'll await that she doesn't recognize the name. This skill is VERY buggy and has not yet worked for me. I even rest Alexa, uninstalled LIFX, and set everything up again.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |null   |1449792000    |VERY Buggy, doesn't work.|false   |null|12 11, 2015|\n",
            "|A27ZJ1NCBFP1HZ|B017O9P72A|Alexa works great for me so far, but I'm also only controlling a single bulb at the moment. Turning on/off, changing colors and adjusting brightness are all easy and quick. That being said, I'm expecting complications as I add more bulbs (hope for the best prepare for the worst, right?)\\n\\nI'd speculate that some other users' frustrations might stem from Alexa not recognizing their bulb or room names. After simplifying my bulb name to 'Lamp' and listing it under a Living Room group (within the LIFx app), I've been able to address it by either category pretty consistently.\\n\"Turn on/off living room lights.\"\\n\"Change lamp light to [color]\"\\n\"Dim living room lights to [X]%\"\\n\\nLike any new tech, you can expect growing pains and bugs early on. Be patient. This skill isn't perfect by any means, but I'd say it's off to a decent start|null   |1449532800    |So Far So Good           |false   |5   |12 8, 2015 |\n",
            "|ACCQIOZMFN4UK |B017O9P72A|Weak!!\\n\\nAlexa doesn't even recognize the name Lifx.\\nIt's a waste of time to even ask her to turn on and off lights                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |null   |1449446400    |Time waster              |false   |11  |12 7, 2015 |\n",
            "|A3KUPJ396OQF78|B017O9P72A|Can only control one of two bulbs from one of two echos                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |null   |1449273600    |Buggy                    |false   |null|12 5, 2015 |\n",
            "|A1U1RE1ZI19E1H|B017O9P72A|this worked great then randomly stopped. please update.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |null   |1517529600    |stopped working          |false   |2   |02 2, 2018 |\n",
            "+--------------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+--------------+-------------------------+--------+----+-----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "🔹 Checking for Null Values:\n",
            "+----------+----+----------+---------+\n",
            "|reviewerID|asin|reviewText|  overall|\n",
            "+----------+----+----------+---------+\n",
            "|         0|   0|    153384|233055327|\n",
            "+----------+----+----------+---------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o80.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:288)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14.0 (TID 10) (3d403495e2e9 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6cc6ace2 : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:245)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:188)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:306)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:116)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:431)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:450)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:485)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n\tat org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:255)\n\t... 42 more\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6cc6ace2 : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:245)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:188)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:306)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:116)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:431)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:450)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:485)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n\tat org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-56173b0eb31a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrecord_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mvalidate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpartition_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Partitioning for performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-77f0803265f9>\u001b[0m in \u001b[0;36mpartition_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# Partitioning by 'overall' rating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overall\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitioned_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✅ Data partitioned and saved to: {partitioned_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1721\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m     def text(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o80.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:288)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14.0 (TID 10) (3d403495e2e9 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6cc6ace2 : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:245)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:188)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:306)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:116)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:431)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:450)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:485)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n\tat org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:255)\n\t... 42 more\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6cc6ace2 : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:245)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:188)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:306)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:116)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:431)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:450)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:485)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n\tat org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automatically Downloading The Log File"
      ],
      "metadata": {
        "id": "KOzpb1ZagPJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download logs file after execution\n",
        "download_logs(log_filename)"
      ],
      "metadata": {
        "id": "Yd7O-7ZE22wF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9420d34a-3c81-446d-e99d-cb8e0317d4d6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading log file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Analysis & Visualization Assignment # 1 - 2"
      ],
      "metadata": {
        "id": "WNjvX00Cw5KT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading All Necessary Libraries"
      ],
      "metadata": {
        "id": "jkSHtB9Sc2rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHgaxeGwcx4x",
        "outputId": "6b05b4fa-5d7a-4a9a-c551-d4b229b26b19"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dash\n",
            "  Downloading dash-2.18.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting Flask<3.1,>=1.0.4 (from dash)\n",
            "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting Werkzeug<3.1 (from dash)\n",
            "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash) (5.24.1)\n",
            "Collecting dash-html-components==2.0.0 (from dash)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting dash-table==5.0.0 (from dash)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash) (8.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash) (4.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dash) (2.32.3)\n",
            "Collecting retrying (from dash)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash) (75.1.0)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (1.9.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug<3.1->dash) (3.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from retrying->dash) (1.17.0)\n",
            "Downloading dash-2.18.2-py3-none-any.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, Werkzeug, retrying, Flask, dash\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "  Attempting uninstall: Flask\n",
            "    Found existing installation: Flask 3.1.0\n",
            "    Uninstalling Flask-3.1.0:\n",
            "      Successfully uninstalled Flask-3.1.0\n",
            "Successfully installed Flask-3.0.3 Werkzeug-3.0.6 dash-2.18.2 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 retrying-1.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All Libraries"
      ],
      "metadata": {
        "id": "2UKkKzIVhDHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pyspark Types\n",
        "from pyspark.sql.types import StructField\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.types import BooleanType\n",
        "from pyspark.sql.types import StructType\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Pyspark Function Modules\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "from pyspark.sql.functions import explode\n",
        "from pyspark.sql.functions import to_date\n",
        "from pyspark.sql.functions import stddev\n",
        "from pyspark.sql.functions import length\n",
        "from pyspark.sql.functions import split\n",
        "from pyspark.sql.functions import count\n",
        "from pyspark.sql.functions import mean\n",
        "from pyspark.sql.functions import when\n",
        "from pyspark.sql.functions import expr\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import min\n",
        "from pyspark.sql.functions import max\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Pyspark Machine Learning Feature Modules\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.ml.feature import RegexTokenizer\n",
        "\n",
        "# Pyspark Machine Learning Clustering Modules\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.clustering import LDA\n",
        "\n",
        "# Pyspark Modules\n",
        "from pyspark.sql import SparkSession\n",
        "import findspark\n",
        "\n",
        "# Dash Dependencies\n",
        "from dash.dependencies import Output\n",
        "from dash.dependencies import Input\n",
        "\n",
        "# Dash Modules\n",
        "from dash import html\n",
        "from dash import dcc\n",
        "import dash\n",
        "\n",
        "# Visualization Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "\n",
        "# Other Necessary Libraries\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "NHKqIXo_hEdq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All Functions"
      ],
      "metadata": {
        "id": "6y9IwZOUhE1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility Functions"
      ],
      "metadata": {
        "id": "eD6p-20AsGAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the processed data\n",
        "def load_transformed_data(spark, path=\"/content/processed_output/\"):\n",
        "    return spark.read.parquet(path)\n",
        "\n",
        "# Get Dataset Shape\n",
        "def get_shape(df):\n",
        "    print(f\"Dataset contains {df.count()} rows and {len(df.columns)} columns.\")\n",
        "\n",
        "# Check Column Types\n",
        "def check_schema(df):\n",
        "    print(\"Schema:\")\n",
        "    df.printSchema()"
      ],
      "metadata": {
        "id": "wchjoScGsHY4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streaming Data Pipeline Functions"
      ],
      "metadata": {
        "id": "ojbMNxWNYE8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting Up Functions"
      ],
      "metadata": {
        "id": "EDnO87gHrRO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Up Spark Streaming\n",
        "def setup_spark():\n",
        "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "    os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "    findspark.init()\n",
        "\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"AmazonReviewsStreaming\") \\\n",
        "        .config(\"spark.driver.memory\", \"4g\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    print(\"Spark Streaming Session Created!\")\n",
        "    return spark\n",
        "\n",
        "\n",
        "# Define Schema for Structured Streaming\n",
        "def get_schema():\n",
        "    return StructType([\n",
        "        StructField(\"reviewerID\", StringType(), True),\n",
        "        StructField(\"asin\", StringType(), True),\n",
        "        StructField(\"reviewText\", StringType(), True),\n",
        "        StructField(\"overall\", IntegerType(), True),\n",
        "        StructField(\"unixReviewTime\", IntegerType(), True),\n",
        "        StructField(\"summary\", StringType(), True),\n",
        "        StructField(\"verified\", BooleanType(), True),\n",
        "        StructField(\"vote\", StringType(), True),\n",
        "        StructField(\"reviewTime\", StringType(), True)\n",
        "    ])"
      ],
      "metadata": {
        "id": "aKksvs_5tMe5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Necessary Functions"
      ],
      "metadata": {
        "id": "Iou-hMM-rUPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the Data (Handle Missing Values & Duplicates)\n",
        "def clean_data(df):\n",
        "    return df.dropDuplicates([\"reviewerID\", \"asin\"]) \\\n",
        "             .dropna(subset=[\"reviewerID\", \"asin\", \"reviewText\", \"overall\"])\n",
        "\n",
        "\n",
        "# Detect Anomalies in Review Ratings (Using Z-score)\n",
        "def detect_anomalies(df):\n",
        "    stats = df.select(mean(col(\"overall\")).alias(\"mean_rating\"),\n",
        "                      stddev(col(\"overall\")).alias(\"stddev_rating\")).collect()\n",
        "\n",
        "    if stats:\n",
        "        mean_rating = stats[0][\"mean_rating\"]\n",
        "        stddev_rating = stats[0][\"stddev_rating\"]\n",
        "\n",
        "        df = df.withColumn(\"is_anomaly\", when((col(\"overall\") - mean_rating) / stddev_rating > 2, True).otherwise(False))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "4QfEw0o5rYeF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Driver Function"
      ],
      "metadata": {
        "id": "9uNSUnJcrYvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Streaming Function to Read Data in Real-Time\n",
        "def start_streaming(spark, schema):\n",
        "    input_dir = \"/content/partitioned_reviews\"\n",
        "    output_dir = \"/content/processed_output\"\n",
        "\n",
        "    # Create directories for streaming simulation\n",
        "    shutil.rmtree(input_dir, ignore_errors=True)\n",
        "    shutil.rmtree(output_dir, ignore_errors=True)\n",
        "    os.makedirs(input_dir)\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "    print(\"Streaming directories created!\")\n",
        "\n",
        "    # Read new files as they arrive\n",
        "    streaming_df = spark.readStream.schema(schema).json(input_dir)\n",
        "\n",
        "    # Apply data cleaning and anomaly detection\n",
        "    cleaned_df = clean_data(streaming_df)\n",
        "    processed_df = detect_anomalies(cleaned_df)\n",
        "\n",
        "    # Write output as Parquet files (real-time updates)\n",
        "    query = processed_df.writeStream \\\n",
        "        .format(\"parquet\") \\\n",
        "        .option(\"checkpointLocation\", \"/content/checkpoints\") \\\n",
        "        .option(\"path\", output_dir) \\\n",
        "        .trigger(processingTime=\"10 seconds\") \\\n",
        "        .start()\n",
        "\n",
        "    print(\"Streaming started! Place new JSON files in `/content/partitioned_reviews`.\")\n",
        "\n",
        "    return query"
      ],
      "metadata": {
        "id": "mEL2Us8KraSi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced EDA Functions"
      ],
      "metadata": {
        "id": "5x011F-4YQyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Understanding & Summary Statistics Functions"
      ],
      "metadata": {
        "id": "_HTCJBSvYVFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Necessary Functions"
      ],
      "metadata": {
        "id": "MSNkgmwisRWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count Missing Values in Each Column\n",
        "def check_missing_values(df):\n",
        "    print(\"Missing Values Count:\")\n",
        "    df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
        "\n",
        "# Compute Summary Statistics\n",
        "def compute_statistics(df):\n",
        "    print(\"Summary Statistics:\")\n",
        "    stats_df = df.select(\n",
        "        mean(col(\"overall\")).alias(\"Mean\"),\n",
        "        expr(\"percentile_approx(overall, 0.5)\").alias(\"Median\"),\n",
        "        expr(\"mode(overall)\").alias(\"Mode\"),  # Requires extra handling in Spark\n",
        "        min(col(\"overall\")).alias(\"Min\"),\n",
        "        max(col(\"overall\")).alias(\"Max\"),\n",
        "        stddev(col(\"overall\")).alias(\"Std Dev\")\n",
        "    )\n",
        "    stats_df.show()"
      ],
      "metadata": {
        "id": "Uko2Ru27WD-N"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Driver Function"
      ],
      "metadata": {
        "id": "WOMrvSsmsUj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run All EDA Steps\n",
        "def run_eda(spark):\n",
        "    df = load_transformed_data(spark)\n",
        "\n",
        "    get_shape(df)\n",
        "    check_schema(df)\n",
        "    check_missing_values(df)\n",
        "    compute_statistics(df)"
      ],
      "metadata": {
        "id": "z0XslFp_sV6D"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus Feature Functions"
      ],
      "metadata": {
        "id": "4CW5dCqCrIZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Bonus Dataset Function"
      ],
      "metadata": {
        "id": "JjtCu7X3s711"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Incorporating bonus dataset\n",
        "def load_bonus_dataset(spark, path=\"/content/omp_serial.json\"):\n",
        "    print(\"\\nLoading and analyzing bonus dataset...\")\n",
        "\n",
        "    schema = StructType([\n",
        "        StructField(\"ID\", StringType(), True),\n",
        "        StructField(\"realId\", StringType(), True),\n",
        "        StructField(\"pragma\", StringType(), True),\n",
        "        StructField(\"label_para\", IntegerType(), True),\n",
        "        StructField(\"label_function_call\", IntegerType(), True),\n",
        "        StructField(\"label_nested_loop\", IntegerType(), True),\n",
        "        StructField(\"label_reduction\", IntegerType(), True),\n",
        "        StructField(\"label_task\", IntegerType(), True),\n",
        "        StructField(\"label_simd\", IntegerType(), True),\n",
        "        StructField(\"label_target\", IntegerType(), True)\n",
        "    ])\n",
        "\n",
        "    df_bonus = spark.read.json(path, schema=schema)\n",
        "\n",
        "    # Count interactions per product\n",
        "    df_bonus.groupBy(\"pragma\").count().orderBy(col(\"count\").desc()).show(5)\n",
        "\n",
        "    return df_bonus"
      ],
      "metadata": {
        "id": "pe4mTB1ns-De"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Anomaly Detection Using Machine Learning Function"
      ],
      "metadata": {
        "id": "ridTnyQ9sbOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect anomalies using machine learning\n",
        "def detect_anomalies_mllib(df):\n",
        "    print(\"\\nDetecting anomalies using Spark MLlib...\")\n",
        "\n",
        "    assembler = VectorAssembler(inputCols=[\"overall\"], outputCol=\"features\")\n",
        "    df_vectorized = assembler.transform(df)\n",
        "\n",
        "    kmeans = KMeans(k=2, seed=1, featuresCol=\"features\", predictionCol=\"cluster\")\n",
        "    model = kmeans.fit(df_vectorized)\n",
        "\n",
        "    df_clustered = model.transform(df_vectorized)\n",
        "    df_anomalies = df_clustered.filter(df_clustered[\"cluster\"] == 1)  # Assuming cluster 1 contains anomalies\n",
        "\n",
        "    print(f\"Anomalies detected: {df_anomalies.count()}\")\n",
        "    return df_anomalies"
      ],
      "metadata": {
        "id": "BMq0DlYPsiDT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating Interactive Dashboard Function"
      ],
      "metadata": {
        "id": "IcRe2y-DsjMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive dashboard\n",
        "def start_dashboard(df):\n",
        "    df_pandas = df.toPandas()\n",
        "\n",
        "    app = dash.Dash(__name__)\n",
        "\n",
        "    app.layout = html.Div([\n",
        "        html.H1(\"Amazon Reviews Interactive Dashboard\"),\n",
        "        dcc.Graph(id=\"review-distribution\", figure=px.box(df_pandas, y=\"overall\", title=\"Review Rating Distribution\")),\n",
        "        dcc.Graph(id=\"top-products\", figure=px.bar(df_pandas.groupby(\"asin\").size().reset_index(name=\"review_count\").nlargest(10, \"review_count\"),\n",
        "                                                   x=\"asin\", y=\"review_count\", title=\"Top 10 Most Reviewed Products\")),\n",
        "        dcc.Graph(id=\"sentiment-distribution\", figure=px.pie(df_pandas, names=\"sentiment\", title=\"Sentiment Distribution\"))\n",
        "    ])\n",
        "\n",
        "    app.run_server(debug=False, port=8050)"
      ],
      "metadata": {
        "id": "eo2r6PpTsnA2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Performance Optimization Function"
      ],
      "metadata": {
        "id": "P4mOiVeCsuQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance optimization\n",
        "def optimize_spark(df):\n",
        "    print(\"\\nOptimizing Spark jobs with caching and partitioning...\")\n",
        "\n",
        "    df.cache()  # Cache for faster access\n",
        "    df = df.repartition(10, \"overall\")  # Partition data by rating for parallel processing\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "RLnjCxIns0kq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "oxggJUE7qRKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run The Streaming Pipeline"
      ],
      "metadata": {
        "id": "xq_2Fk3QqSHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Streaming Pipeline\n",
        "\n",
        "spark = setup_spark()\n",
        "schema = get_schema()\n",
        "query = start_streaming(spark, schema)\n",
        "\n",
        "# Let the streaming run for 2 minutes before stopping (for testing)\n",
        "query.awaitTermination(120)\n",
        "query.stop()\n",
        "\n",
        "print(\"Streaming stopped.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrNoROIkqV_u",
        "outputId": "f830268b-bebc-45d6-e13e-5fd808740bb6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Streaming Session Created!\n",
            "Streaming directories created!\n",
            "Streaming started! Place new JSON files in `/content/partitioned_reviews`.\n",
            "Streaming stopped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Data Understanding & Summary Statistics EDA"
      ],
      "metadata": {
        "id": "UCfUt_f4qctJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute EDA\n",
        "run_eda(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BXUDih3qkDg",
        "outputId": "7570e958-92bc-41ce-b19e-915368c98305"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset contains 233055327 rows and 308 columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do Query Based EDA"
      ],
      "metadata": {
        "id": "1fQ6X37iYb1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_transformed_data(spark)\n",
        "\n",
        "# Top 5 Most Reviewed Products\n",
        "print(\"\\nTop 5 Most Reviewed Products:\")\n",
        "df.groupBy(\"asin\").agg(count(\"reviewText\").alias(\"review_count\")) \\\n",
        "  .orderBy(col(\"review_count\").desc()) \\\n",
        "  .show(5, truncate=False)\n",
        "\n",
        "# Average Review Ratings by Category\n",
        "print(\"\\nAverage Review Ratings by Category:\")\n",
        "df.groupBy(\"asin\").agg(avg(\"overall\").alias(\"avg_rating\")) \\\n",
        "  .orderBy(col(\"avg_rating\").desc()) \\\n",
        "  .show(10, truncate=False)\n",
        "\n",
        "# Correlation Between Review Length & Rating\n",
        "print(\"\\nCorrelation Between Review Length & Rating:\")\n",
        "df = df.withColumn(\"review_length\", length(col(\"reviewText\")))\n",
        "df.select(F.corr(\"review_length\", \"overall\").alias(\"correlation_review_length_rating\")).show()\n",
        "\n",
        "# Review Trends Over Time (Time-Series Analysis)\n",
        "print(\"\\nReview Trends Over Time:\")\n",
        "df.withColumn(\"review_date\", to_date(col(\"reviewTime\"), \"MM dd, yyyy\")) \\\n",
        "  .groupBy(\"review_date\") \\\n",
        "  .agg(count(\"reviewText\").alias(\"review_count\")) \\\n",
        "  .orderBy(col(\"review_date\")) \\\n",
        "  .show(10, truncate=False)\n",
        "\n",
        "# Percentage of Reviews Mentioning 'Refund', 'Return', or 'Defective'\n",
        "print(\"\\nPercentage of Reviews Mentioning 'Refund', 'Return', or 'Defective':\")\n",
        "keyword_df = df.withColumn(\"contains_keyword\",\n",
        "                           when(col(\"reviewText\").rlike(\"(?i)refund|return|defective\"), 1).otherwise(0))\n",
        "total_reviews = df.count()\n",
        "keyword_reviews = keyword_df.filter(col(\"contains_keyword\") == 1).count()\n",
        "percentage = (keyword_reviews / total_reviews) * 100 if total_reviews > 0 else 0\n",
        "print(f\"🔸 {percentage:.2f}% of reviews mention refund, return, or defective.\")\n",
        "\n",
        "# Most Polarized Products (1-Star vs. 5-Star)\n",
        "print(\"\\nMost Polarized Products (Highest 1-Star and 5-Star Reviews):\")\n",
        "df.filter((col(\"overall\") == 1) | (col(\"overall\") == 5)) \\\n",
        "  .groupBy(\"asin\") \\\n",
        "  .agg(count(when(col(\"overall\") == 1, True)).alias(\"one_star_reviews\"),\n",
        "       count(when(col(\"overall\") == 5, True)).alias(\"five_star_reviews\")) \\\n",
        "  .orderBy((col(\"one_star_reviews\") + col(\"five_star_reviews\")).desc()) \\\n",
        "  .show(5, truncate=False)\n",
        "\n",
        "# Verified vs. Non-Verified Purchase Ratings\n",
        "print(\"\\nVerified vs. Non-Verified Purchase Ratings:\")\n",
        "df.groupBy(\"verified\") \\\n",
        "  .agg(avg(\"overall\").alias(\"avg_rating\")) \\\n",
        "  .show()\n",
        "\n",
        "# Fake-Looking Reviews (Excessive Word Repetition)\n",
        "print(\"\\nFake-Looking Reviews (Excessive Word Repetition):\")\n",
        "df.withColumn(\"word_count\", expr(\"size(split(reviewText, ' '))\")) \\\n",
        "  .withColumn(\"unique_word_count\", expr(\"size(array_distinct(split(reviewText, ' ')))\")) \\\n",
        "  .withColumn(\"word_repetition_ratio\", col(\"word_count\") / col(\"unique_word_count\")) \\\n",
        "  .orderBy(col(\"word_repetition_ratio\").desc()) \\\n",
        "  .select(\"asin\", \"reviewText\", \"word_repetition_ratio\") \\\n",
        "  .show(5, truncate=False)"
      ],
      "metadata": {
        "id": "vYVYaqW3WEC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do Text Analysis EDA (NLP)"
      ],
      "metadata": {
        "id": "nYlHf8yKYdg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Processed Data\n",
        "df = load_transformed_data(spark)\n",
        "\n",
        "# 1. Most Frequent Words (Word Cloud)\n",
        "print(\"\\nGenerating Word Cloud for Most Frequent Words...\")\n",
        "\n",
        "# Tokenize words\n",
        "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "df_tokenized = tokenizer.transform(df)\n",
        "\n",
        "# Remove stopwords\n",
        "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "df_cleaned = stopwords_remover.transform(df_tokenized)\n",
        "\n",
        "# Explode words into separate rows for frequency counting\n",
        "df_exploded = df_cleaned.select(explode(col(\"filtered_words\")).alias(\"word\"))\n",
        "df_word_count = df_exploded.groupBy(\"word\").count().orderBy(col(\"count\").desc())\n",
        "\n",
        "# Convert to Pandas for WordCloud\n",
        "top_words = df_word_count.limit(100).toPandas()\n",
        "word_freq_dict = dict(zip(top_words[\"word\"], top_words[\"count\"]))\n",
        "\n",
        "# Generate Word Cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_freq_dict)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Sentiment Analysis (Positive, Neutral, Negative)\n",
        "print(\"\\nPerforming Sentiment Analysis...\")\n",
        "\n",
        "df_sentiment = df.withColumn(\"sentiment\",\n",
        "                             when(col(\"overall\") >= 4, \"Positive\")\n",
        "                             .when(col(\"overall\") == 3, \"Neutral\")\n",
        "                             .otherwise(\"Negative\"))\n",
        "\n",
        "# Display Sentiment Counts\n",
        "df_sentiment.groupBy(\"sentiment\").count().show()\n",
        "\n",
        "# Topic Modeling using LDA (Bonus)\n",
        "print(\"\\nPerforming Topic Modeling (LDA)...\")\n",
        "\n",
        "# Convert text into numerical format for LDA\n",
        "vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"features\", vocabSize=1000, minDF=5)\n",
        "df_vectorized = vectorizer.fit(df_cleaned).transform(df_cleaned)\n",
        "\n",
        "# Train LDA Model\n",
        "lda = LDA(k=5, maxIter=10, featuresCol=\"features\")\n",
        "lda_model = lda.fit(df_vectorized)\n",
        "\n",
        "# Show topics\n",
        "topics = lda_model.describeTopics(5)\n",
        "topics.show(truncate=False)"
      ],
      "metadata": {
        "id": "8fk31gOfXI4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do Correlation & Business Insights"
      ],
      "metadata": {
        "id": "uztjXnajYiKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Processed Data\n",
        "df = load_transformed_data(spark)\n",
        "\n",
        "# Highly Reviewed Products & Top-Rated Categories\n",
        "print(\"\\nHighly Reviewed Products:\")\n",
        "df.groupBy(\"asin\") \\\n",
        "  .agg(count(\"reviewText\").alias(\"review_count\"), avg(\"overall\").alias(\"avg_rating\")) \\\n",
        "  .orderBy(col(\"review_count\").desc()) \\\n",
        "  .show(10, truncate=False)\n",
        "\n",
        "# Detect Words That Correlate with Positive/Negative Reviews\n",
        "print(\"\\nFinding Words That Correlate with Review Sentiment...\")\n",
        "\n",
        "# Tokenize words\n",
        "df_tokenized = df.withColumn(\"words\", split(col(\"reviewText\"), \" \"))\n",
        "\n",
        "# Explode words into separate rows\n",
        "df_exploded = df_tokenized.select(col(\"overall\"), explode(col(\"words\")).alias(\"word\"))\n",
        "\n",
        "# Aggregate word frequency per sentiment\n",
        "df_word_sentiment = df_exploded.groupBy(\"word\") \\\n",
        "    .agg(avg(\"overall\").alias(\"avg_rating\"), count(\"word\").alias(\"word_count\")) \\\n",
        "    .filter(col(\"word_count\") > 50) \\\n",
        "    .orderBy(col(\"avg_rating\").desc())\n",
        "\n",
        "df_word_sentiment.show(10, truncate=False)\n",
        "\n",
        "# Correlation Heatmap for Product Success Factors\n",
        "print(\"\\nCorrelation Heatmap for Product Success Factors\")\n",
        "\n",
        "# Convert Spark DataFrame to Pandas\n",
        "df_corr = df.select(\"overall\", \"verified\", \"vote\").toPandas()\n",
        "\n",
        "# Convert categorical boolean to numeric\n",
        "df_corr[\"verified\"] = df_corr[\"verified\"].astype(int)\n",
        "df_corr[\"vote\"] = pd.to_numeric(df_corr[\"vote\"], errors=\"coerce\")\n",
        "\n",
        "# Compute Correlation Matrix\n",
        "correlation_matrix = df_corr.corr()\n",
        "\n",
        "# Plot Heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap - Product Success Factors\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9k84PJv3XI67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Interactive Dash App With 8 Visualizations"
      ],
      "metadata": {
        "id": "ZyYaHE4QYmp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Processed Data\n",
        "df = load_transformed_data(spark).toPandas()\n",
        "\n",
        "# Convert review time to date format\n",
        "df[\"reviewTime\"] = pd.to_datetime(df[\"reviewTime\"], errors=\"coerce\")\n",
        "\n",
        "# Create output folder\n",
        "os.makedirs(\"visualizations\", exist_ok=True)\n",
        "\n",
        "# Bar Chart - Top 10 Most Reviewed Products\n",
        "fig1 = px.bar(\n",
        "    df.groupby(\"asin\").size().reset_index(name=\"review_count\").nlargest(10, \"review_count\"),\n",
        "    x=\"asin\", y=\"review_count\", title=\"Top 10 Most Reviewed Products\"\n",
        ")\n",
        "fig1.write_html(\"visualizations/top_reviewed_products.html\")\n",
        "\n",
        "# Box Plot - Distribution of Ratings\n",
        "fig2 = px.box(df, y=\"overall\", title=\"Distribution of Review Ratings\")\n",
        "fig2.write_html(\"visualizations/review_distribution.html\")\n",
        "\n",
        "# Scatter Plot - Review Length vs. Rating\n",
        "df[\"review_length\"] = df[\"reviewText\"].str.len()\n",
        "fig3 = px.scatter(df, x=\"review_length\", y=\"overall\", title=\"Review Length vs. Rating\")\n",
        "fig3.write_html(\"visualizations/review_length_vs_rating.html\")\n",
        "\n",
        "# Time-Series - Review Trends Over Time\n",
        "fig4 = px.line(\n",
        "    df.groupby(df[\"reviewTime\"].dt.to_period(\"M\")).size().reset_index(name=\"review_count\"),\n",
        "    x=\"reviewTime\", y=\"review_count\", title=\"Review Trends Over Time\"\n",
        ")\n",
        "fig4.write_html(\"visualizations/review_trends.html\")\n",
        "\n",
        "# Pie Chart - Sentiment Distribution\n",
        "df[\"sentiment\"] = df[\"overall\"].apply(lambda x: \"Positive\" if x >= 4 else \"Neutral\" if x == 3 else \"Negative\")\n",
        "fig5 = px.pie(df, names=\"sentiment\", title=\"Sentiment Distribution in Reviews\")\n",
        "fig5.write_html(\"visualizations/sentiment_distribution.html\")\n",
        "\n",
        "# Bar Chart - Verified vs. Non-Verified Purchase Ratings\n",
        "fig6 = px.bar(\n",
        "    df.groupby(\"verified\")[\"overall\"].mean().reset_index(),\n",
        "    x=\"verified\", y=\"overall\", title=\"Verified vs. Non-Verified Purchase Ratings\"\n",
        ")\n",
        "fig6.write_html(\"visualizations/verified_vs_non_verified.html\")\n",
        "\n",
        "# Heatmap - Correlation of Features\n",
        "fig7 = px.imshow(df[[\"overall\", \"review_length\"]].corr(), text_auto=True, title=\"Feature Correlation Heatmap\")\n",
        "fig7.write_html(\"visualizations/correlation_heatmap.html\")\n",
        "\n",
        "# Word Cloud - Most Frequent Words\n",
        "\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(\" \".join(df[\"reviewText\"].dropna()))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"visualizations/word_cloud.png\", bbox_inches=\"tight\")\n",
        "\n",
        "# Create Dash App for Interactive Exploration\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "app.layout = html.Div([\n",
        "    html.H1(\"Amazon Reviews - Interactive Data Analysis\"),\n",
        "    dcc.Graph(figure=fig1),\n",
        "    dcc.Graph(figure=fig2),\n",
        "    dcc.Graph(figure=fig3),\n",
        "    dcc.Graph(figure=fig4),\n",
        "    dcc.Graph(figure=fig5),\n",
        "    dcc.Graph(figure=fig6),\n",
        "    dcc.Graph(figure=fig7),\n",
        "    html.Img(src=\"visualizations/word_cloud.png\", style={\"width\": \"70%\"})\n",
        "])\n",
        "\n",
        "# Run the Dash App\n",
        "app.run_server(debug=True, port=8050)"
      ],
      "metadata": {
        "id": "mhw7ppsoXI9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run The Bonus Feature Pipeline"
      ],
      "metadata": {
        "id": "r0Co70CiYqxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = setup_spark()\n",
        "schema = get_schema()\n",
        "df = load_transformed_data(spark)\n",
        "\n",
        "# Apply optimizations\n",
        "df = optimize_spark(df)\n",
        "\n",
        "# Detect anomalies\n",
        "anomalies_df = detect_anomalies_mllib(df)\n",
        "\n",
        "# Load bonus dataset\n",
        "df_bonus = load_bonus_dataset(spark)\n",
        "\n",
        "# Start interactive dashboard\n",
        "start_dashboard(df)"
      ],
      "metadata": {
        "id": "h-A0AnIjXI_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Thank You!***"
      ],
      "metadata": {
        "id": "DAFVxDmmtTZV"
      }
    }
  ]
}